{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf37f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import poisson\n",
    "import copy\n",
    "import textwrap\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "def SumNormal(u, Z, weight):\n",
    "    \"\"\"\n",
    "    Calculates the mean and variance of a linear combination of normally distributed variables.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    u : np.ndarray, shape (n, 1)\n",
    "        Mean vector of the multivariate normal distribution.\n",
    "\n",
    "    Z : np.ndarray, shape (n, n)\n",
    "        Covariance matrix of the multivariate normal distribution.\n",
    "\n",
    "    weight : np.ndarray, shape (1, n)\n",
    "        Weight vector used to compute the linear combination.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    u_N : float\n",
    "        The mean of the resulting linear combination.\n",
    "\n",
    "    sigma_N : float\n",
    "        The variance of the resulting linear combination.\n",
    "    \"\"\"\n",
    "    # Compute the weighted mean: u_N = weight · u\n",
    "    u_N = np.dot(weight, u)[0, 0]\n",
    "\n",
    "    # Compute the weighted variance: sigma_N = weight · Z · weight.T\n",
    "    sigma_N = np.dot(np.dot(weight, Z), weight.transpose())[0, 0]\n",
    "\n",
    "    return u_N, sigma_N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ac0cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_coverage_rate(sampled_x, sampled_y, bins, lower_line_data, higher_line_data):\n",
    "    \"\"\"\n",
    "    Compute whether sampled points fall between the upper and lower boundaries\n",
    "    of each bin, and return the coverage statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sampled_x : 1D array\n",
    "        x-coordinates of the sampled points.\n",
    "    sampled_y : 1D array\n",
    "        y-coordinates of the sampled points.\n",
    "    bins : 1D array\n",
    "        Bin edges along the x-axis.\n",
    "    lower_line_data : 1D array\n",
    "        Lower boundary y-values for each bin (length = len(bins)).\n",
    "    higher_line_data : 1D array\n",
    "        Upper boundary y-values for each bin (length = len(bins)).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    coverage_rate : float\n",
    "        Proportion of points that fall within the boundaries.\n",
    "    covered_count : int\n",
    "        Number of points within the boundaries.\n",
    "    total_count : int\n",
    "        Total number of sampled points.\n",
    "    \"\"\"\n",
    "\n",
    "    sampled_x = np.array(sampled_x)\n",
    "    sampled_y = np.array(sampled_y)\n",
    "    covered_count = 0\n",
    "\n",
    "    for i in range(len(bins) - 1):\n",
    "        x_start = bins[i]\n",
    "        x_end = bins[i + 1]\n",
    "\n",
    "        # Fit the lower boundary line\n",
    "        y_lower_start = lower_line_data[i]\n",
    "        y_lower_end = lower_line_data[i + 1]\n",
    "        slope_lower = (y_lower_end - y_lower_start) / (x_end - x_start)\n",
    "        intercept_lower = y_lower_start - slope_lower * x_start\n",
    "\n",
    "        # Fit the upper boundary line\n",
    "        y_upper_start = higher_line_data[i]\n",
    "        y_upper_end = higher_line_data[i + 1]\n",
    "        slope_upper = (y_upper_end - y_upper_start) / (x_end - x_start)\n",
    "        intercept_upper = y_upper_start - slope_upper * x_start\n",
    "\n",
    "        # Identify points belonging to the current bin\n",
    "        bin_mask = (sampled_x >= x_start) & (sampled_x < x_end)\n",
    "        x_bin = sampled_x[bin_mask]\n",
    "        y_bin = sampled_y[bin_mask]\n",
    "\n",
    "        # Compute the predicted lower and upper boundary values for each point\n",
    "        y_lower_pred = slope_lower * x_bin + intercept_lower\n",
    "        y_upper_pred = slope_upper * x_bin + intercept_upper\n",
    "\n",
    "        # Determine whether points fall between the boundaries\n",
    "        covered_mask = (y_bin >= y_lower_pred) & (y_bin <= y_upper_pred)\n",
    "        covered_count += np.sum(covered_mask)\n",
    "\n",
    "    total_count = len(sampled_x)\n",
    "    coverage_rate = covered_count / total_count\n",
    "    return coverage_rate, covered_count, total_count\n",
    "\n",
    "def mkdir_silent(directory_name):\n",
    "    try:\n",
    "        os.mkdir(directory_name)\n",
    "        print(f\"Directory '{directory_name}' created successfully.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Directory '{directory_name}' already exists.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Global styling configuration (set once)\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.family'] = 'Arial'\n",
    "mpl.rcParams['axes.linewidth'] = 0.8\n",
    "mpl.rcParams['axes.labelsize'] = 9\n",
    "mpl.rcParams['xtick.labelsize'] = 8\n",
    "mpl.rcParams['ytick.labelsize'] = 8\n",
    "mpl.rcParams['legend.fontsize'] = 8\n",
    "\n",
    "def plot_publication_scatter(\n",
    "        sampled_x, sampled_y,\n",
    "        bins, u_line_data,\n",
    "        lower_beads, upper_beads,\n",
    "        lower_cells, upper_cells,\n",
    "        xlabel, ylabel, title_text, save_path,\n",
    "        ymin=None, ymax=None):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 4), dpi=300)\n",
    "\n",
    "   \n",
    "    ax.scatter(sampled_x, sampled_y, s=2, alpha=0.25,\n",
    "               color=\"#6A8A82\", edgecolor=\"none\")\n",
    "\n",
    "    # prediction interval（beads）\n",
    "    ax.plot(bins, lower_beads, color=\"#DD8452\", lw=1.2)\n",
    "    ax.plot(bins, upper_beads, color=\"#DD8452\", lw=1.2,\n",
    "            label=\"Beads 95% CI\")\n",
    "\n",
    "    # prediction interval（cells）\n",
    "    ax.plot(bins, lower_cells, color=\"#4C72B0\", lw=1.2)\n",
    "    ax.plot(bins, upper_cells, color=\"#4C72B0\", lw=1.2,\n",
    "            label=\"Cells 95% CI\")\n",
    "\n",
    "    # mean line\n",
    "    ax.plot(bins, u_line_data, color=\"black\", lw=1.0, ls=\"--\",\n",
    "            label=\"Mean\")\n",
    "\n",
    "\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "    # # Automatically set the y-axis range (if not provided)\n",
    "\n",
    "    if ymin is None or ymax is None:\n",
    "        all_y = np.concatenate([\n",
    "            np.array(sampled_y),\n",
    "            np.array(u_line_data),\n",
    "            np.array(lower_beads),\n",
    "            np.array(upper_beads),\n",
    "            np.array(lower_cells),\n",
    "            np.array(upper_cells)\n",
    "        ])\n",
    "\n",
    "        ymin_auto = np.min(all_y)\n",
    "        ymax_auto = np.max(all_y)\n",
    "\n",
    "        ymin = ymin if ymin is not None else ymin_auto\n",
    "        ymax = ymax if ymax is not None else ymax_auto\n",
    "\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    if ymin is None:\n",
    "        ymin = min(sampled_y)  # or np.min(sampled_y)\n",
    "    if ymax is None:\n",
    "        ymax = max(sampled_y)  # or np.max(sampled_y)\n",
    "\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    ax.grid(alpha=0.15, lw=0.5)\n",
    "\n",
    "    ax.legend(\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1.18),\n",
    "        ncol=3,\n",
    "        frameon=False)\n",
    "\n",
    "    fig.suptitle(title_text, fontsize=9, y=1.02)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(save_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f962000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Data dir and Output dir\n",
    "Raw_Data_dir = 'E:/ResidualModel/extra_17_pinv/extra_17_Xenith/extra_17_flow_channel_cor_spectrum_external/Data'\n",
    "Raw_Folder_dir = 'E:/ResidualModel/extra_17_pinv/extra_17_Xenith/extra_17_flow_channel_cor_spectrum_external/Output'\n",
    "Unmix_dir = 'E:/ResidualModel/extra_17_pinv/extra_17_Xenith/extra_17_flow_channel_cor_spectrum_beads/Output'\n",
    "Par_beads_dir = 'E:/ResidualModel/extra_17_pinv/extra_17_Xenith/extra_17_flow_channel_cor_spectrum_beads/Output'\n",
    "Par_cells_dir = 'E:/ResidualModel/extra_17_pinv/extra_17_Xenith/extra_17_flow_channel_cor_spectrum_cells/Output'\n",
    "Output_dir = 'E:/ResidualModel/extra_17_flow_channel_cor_spectrum/Output4'\n",
    "\n",
    "Data_string = \"_External_\"#_External_\n",
    "Unmix_string = \"_Bead_\"\n",
    "Bead_string = \"_Bead_\"\n",
    "Cell_string = \"_Cell_\"\n",
    "\n",
    "Fluor_unmix_list = os.listdir(Unmix_dir)\n",
    "Fluor_beads_list = [s.replace(Unmix_string, Bead_string) for s in Fluor_unmix_list]\n",
    "Fluor_cells_list = [s.replace(Unmix_string, Cell_string) for s in Fluor_unmix_list]\n",
    "Raw_Folder_list = os.listdir(Raw_Folder_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65d703ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "[12]\n"
     ]
    }
   ],
   "source": [
    "Output_dir_root = Output_dir\n",
    "all_fluors = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]\n",
    "#scc_fluors = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]#internal\n",
    "scc_fluors = [2,3,4,5,6,7,8,9,11,12,14,15,18,19,20,22,26,27]#external\n",
    "scc_fluors = [12]#'SCC_Bead_BV510_CD3'\n",
    "#scc_fluors = [26]#external\n",
    "print(all_fluors)\n",
    "print(scc_fluors)\n",
    "Fluor_list = Fluor_unmix_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5f87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'E:/ResidualModel/extra_17_flow_channel_cor_spectrum/Output4/SCC_Bead_BV510_CD3' created successfully.\n",
      "SCC_Bead_BV510_CD3\n",
      "                             0\n",
      "0          SCC_Bead_AF532_CD45\n",
      "1   SCC_Bead_APCeFluor780_CD34\n",
      "2         SCC_Bead_APC_TCRVa72\n",
      "3        SCC_Bead_BUV395_NKG2D\n",
      "4         SCC_Bead_BUV496_CD19\n",
      "5         SCC_Bead_BUV563_CD56\n",
      "6        SCC_Bead_BUV615_CD161\n",
      "7        SCC_Bead_BUV661_CD127\n",
      "8        SCC_Bead_BUV737_NKp30\n",
      "9         SCC_Bead_BUV805_CD16\n",
      "10        SCC_Bead_BV421_NKG2C\n",
      "11        SCC_Bead_BV480_KLRG1\n",
      "12          SCC_Bead_BV570_CD8\n",
      "13         SCC_Bead_BV650_CD38\n",
      "14         SCC_Bead_BV750_CD69\n",
      "15  SCC_Bead_BV785_TCRVa24Ja18\n",
      "16     SCC_Bead_eFluor450_CD57\n",
      "17           SCC_Bead_FITC_CD2\n",
      "18    SCC_Bead_NFB61070S_HLADR\n",
      "19         SCC_Bead_NFR700_CD4\n",
      "20        SCC_Bead_PECy55_CD25\n",
      "21        SCC_Bead_PECy5_CRTH2\n",
      "22  SCC_Bead_PEDazzle594_NKG2A\n",
      "23  SCC_Bead_PerCPVio700_TCRrd\n",
      "24   SCC_Bead_PEVio770_TCRVb11\n",
      "25          SCC_Bead_PE_KIRDL1\n",
      "26        SCC_Bead_RB780_NKp46\n",
      "27        SCC_Bead_SB600_CD244\n",
      "28         SCC_Bead_SB702_cKit\n",
      "29        SCC_Cell_LDNIR876_LD\n",
      "SCC_Bead_BV510_CD3\n",
      "AF\n",
      "SCC_Bead_AF532_CD45\n",
      "SCC_Bead_APCeFluor780_CD34\n",
      "SCC_Bead_APC_TCRVa72\n",
      "SCC_Bead_BUV395_NKG2D\n",
      "SCC_Bead_BUV496_CD19\n",
      "SCC_Bead_BUV563_CD56\n",
      "SCC_Bead_BUV615_CD161\n",
      "SCC_Bead_BUV661_CD127\n",
      "SCC_Bead_BUV737_NKp30\n",
      "SCC_Bead_BUV805_CD16\n",
      "SCC_Bead_BV421_NKG2C\n",
      "SCC_Bead_BV480_KLRG1\n",
      "SCC_Bead_BV570_CD8\n",
      "SCC_Bead_BV650_CD38\n",
      "SCC_Bead_BV750_CD69\n",
      "SCC_Bead_BV785_TCRVa24Ja18\n",
      "SCC_Bead_eFluor450_CD57\n",
      "SCC_Bead_FITC_CD2\n",
      "SCC_Bead_NFB61070S_HLADR\n",
      "SCC_Bead_NFR700_CD4\n",
      "SCC_Bead_PECy55_CD25\n",
      "SCC_Bead_PECy5_CRTH2\n",
      "SCC_Bead_PEDazzle594_NKG2A\n",
      "SCC_Bead_PerCPVio700_TCRrd\n",
      "SCC_Bead_PEVio770_TCRVb11\n",
      "SCC_Bead_PE_KIRDL1\n",
      "SCC_Bead_RB780_NKp46\n",
      "SCC_Bead_SB600_CD244\n",
      "SCC_Bead_SB702_cKit\n",
      "SCC_Cell_LDNIR876_LD\n"
     ]
    }
   ],
   "source": [
    "for i_scc in scc_fluors:\n",
    "\n",
    "    i_folder = i_scc\n",
    "    added_fluors = all_fluors.copy()\n",
    "    added_fluors.pop(i_folder)\n",
    "\n",
    "    Output_dir = Output_dir_root+\"/\"+Fluor_list[i_folder]\n",
    "    mkdir_silent(Output_dir)\n",
    "\n",
    "    \n",
    "    Par_dir = Unmix_dir\n",
    "    Fluor_list = Fluor_unmix_list\n",
    "\n",
    "    print(Fluor_list[i_folder])\n",
    "\n",
    "    #prepare A_pinv\n",
    "    # 1 fluor and 1 af\n",
    "    sig = pd.read_csv(Par_dir+\"/\"+Fluor_list[i_folder]+\"/sig.csv\",encoding=\"utf-8\",index_col=0)\n",
    "    neg_sig = pd.read_csv(Par_dir+\"/\"+Fluor_list[i_folder]+\"/neg_sig.csv\",encoding=\"utf-8\",index_col=0)\n",
    "\n",
    "    neg_sig = neg_sig/max(neg_sig.iloc[:,0])\n",
    "    A_df = pd.concat([sig, neg_sig], axis=1)\n",
    "    A_df.columns = [Fluor_list[i_folder], 'AF']\n",
    "\n",
    "    #add fluors to A_df\n",
    "    print(pd.DataFrame([Fluor_list[f] for f in added_fluors]))\n",
    "    for i_add in added_fluors:\n",
    "        add_sig = pd.read_csv(Par_dir+\"/\"+Fluor_list[i_add]+\"/sig.csv\",encoding=\"utf-8\",index_col=0)\n",
    "        add_sig.columns = [Fluor_list[i_add]]\n",
    "        A_df = pd.concat([A_df, add_sig], axis=1)\n",
    "\n",
    "    #Calculate A_pinv\n",
    "    A_pinv = np.linalg.pinv(A_df.values)\n",
    "\n",
    "\n",
    "    coverage_results_beads_corrected = []\n",
    "    coverage_results_cells_corrected = []\n",
    "    coverage_results_beads_uncorrected = []\n",
    "    coverage_results_cells_uncorrected = []\n",
    "    x_idx = 0 #always 0\n",
    "    for y_idx in range(0,32):\n",
    "        print(A_df.columns[y_idx])\n",
    "\n",
    "        #Step 1 real unmixed results\n",
    "        filtered_list = [s for s in Raw_Folder_list if \"_\"+Fluor_list[i_folder].split(\"_\")[2]+\"_\" in s]\n",
    "        file_sample = filtered_list[0]+ \"_sample.fcs.pkl\"\n",
    "        path = Raw_Data_dir+\"/\"+file_sample\n",
    "        data_sample = pd.read_pickle(path)  \n",
    "        selected_channels = data_sample.columns[19:70]\n",
    "        data_sample = data_sample[selected_channels]\n",
    "        data_sample = data_sample.reset_index(drop=True)\n",
    "        data_sample = np.array(data_sample).transpose()\n",
    "\n",
    "        unmixed_data = np.dot(A_pinv,data_sample)\n",
    "\n",
    "\n",
    "        #Step 2 prepare unmixed data for plot\n",
    "        sample_size = 100\n",
    "        sample_bin = 60\n",
    "        # Extract x and y coordinates\n",
    "        x = unmixed_data[x_idx]\n",
    "        y = unmixed_data[y_idx]\n",
    "\n",
    "        # Compute the 5% and 95% quantiles for x and y\n",
    "        x_lower, x_upper = np.percentile(x, 0.01), np.percentile(x, 99.99)\n",
    "        y_lower, y_upper = np.percentile(y, 0), np.percentile(y, 100)\n",
    "        #y_lower, y_upper = np.percentile(y, 1), np.percentile(y, 99)\n",
    "\n",
    "        # Create a mask to filter points within the specified range\n",
    "        mask = (x >= x_lower) & (x <= x_upper) & (y >= y_lower) & (y <= y_upper)\n",
    "        if Fluor_list[i_folder] == 'SCC_Bead_BV510_CD3':\n",
    "            mask = (x >= 100) & (x <= x_upper) & (y >= y_lower) & (y <= y_upper) #remove irregular shape negative population\n",
    "\n",
    "\n",
    "        # apply mask\n",
    "        x_filtered = x[mask]\n",
    "        y_filtered = y[mask]\n",
    "\n",
    "        bins = np.linspace(x_filtered.min(), x_filtered.max(), sample_bin+1)\n",
    "\n",
    "        # save sampled data\n",
    "        sampled_x = []\n",
    "        sampled_y = []\n",
    "\n",
    "        # Sample 100 points from each bin with replacement\n",
    "        for i in range(sample_bin):\n",
    "            bin_mask = (x_filtered >= bins[i]) & (x_filtered < bins[i+1])\n",
    "            x_bin = x_filtered[bin_mask]\n",
    "            y_bin = y_filtered[bin_mask]\n",
    "            if len(x_bin) > 0:\n",
    "                indices = np.random.choice(len(x_bin), size=sample_size, replace=True)\n",
    "                sampled_x.extend(x_bin[indices])\n",
    "                sampled_y.extend(y_bin[indices])\n",
    "            #else:\n",
    "                #print(i)\n",
    "                #print(len(x_bin))\n",
    "\n",
    "        #correct with compensation\n",
    "        model = TheilSenRegressor()\n",
    "        model.fit(np.array(sampled_x).reshape(-1, 1), sampled_y)\n",
    "        predicted_y = model.predict(np.array(sampled_x).reshape(-1, 1))\n",
    "        sampled_y_corrected = sampled_y - predicted_y\n",
    "        sampled_x_corrected = sampled_x.copy()\n",
    "\n",
    "        # mask sampled_y_corrected\n",
    "        sampled_x_corrected = np.array(sampled_x_corrected)\n",
    "        sampled_y_corrected_lower, sampled_y_corrected_upper = np.percentile(sampled_y_corrected, 1), np.percentile(sampled_y_corrected, 99)\n",
    "        mask_sampled_y_corrected = (sampled_y_corrected >= sampled_y_corrected_lower) & (sampled_y_corrected <= sampled_y_corrected_upper)\n",
    "        sampled_y_corrected = sampled_y_corrected[mask_sampled_y_corrected]\n",
    "        sampled_y_corrected = sampled_y_corrected.tolist()\n",
    "        sampled_x_corrected = sampled_x_corrected[mask_sampled_y_corrected]\n",
    "        sampled_x_corrected = sampled_x_corrected.tolist()\n",
    "\n",
    "\n",
    "        # mask sampled_y\n",
    "        sampled_x = np.array(sampled_x)\n",
    "        sampled_y = np.array(sampled_y)\n",
    "        sampled_y_lower, sampled_y_upper = np.percentile(sampled_y, 1), np.percentile(sampled_y, 99)\n",
    "        mask_sampled_y = (sampled_y >= sampled_y_lower) & (sampled_y <= sampled_y_upper)\n",
    "        sampled_y = sampled_y[mask_sampled_y]\n",
    "        sampled_y = sampled_y.tolist()\n",
    "        sampled_x = sampled_x[mask_sampled_y]\n",
    "        sampled_x = sampled_x.tolist()\n",
    "\n",
    "        #Step 3 prepare predicted data from beads par\n",
    "\n",
    "        intercepts_B_vs_mean = np.load(Par_beads_dir+\"/\"+Fluor_beads_list[i_folder]+\"/intercepts_B_vs_mean.npy\")\n",
    "        slopes_B_vs_mean = np.load(Par_beads_dir+\"/\"+Fluor_beads_list[i_folder]+\"/slopes_B_vs_mean.npy\")\n",
    "\n",
    "        intercepts_B_vs_cov_beads = np.load(Par_beads_dir+\"/\"+Fluor_beads_list[i_folder]+\"/intercepts_B_vs_cov.npy\")\n",
    "        slopes_B_vs_cov_beads = np.load(Par_beads_dir+\"/\"+Fluor_beads_list[i_folder]+\"/slopes_B_vs_cov.npy\")\n",
    "\n",
    "        intercepts_B_vs_cov_cells = np.load(Par_cells_dir+\"/\"+Fluor_cells_list[i_folder]+\"/intercepts_B_vs_cov.npy\")\n",
    "        slopes_B_vs_cov_cells = np.load(Par_cells_dir+\"/\"+Fluor_cells_list[i_folder]+\"/slopes_B_vs_cov.npy\")\n",
    "\n",
    "        pred_array_beads = np.zeros((A_pinv.shape[0],3,len(bins)))# 0: mean, 1: std, 2: bins\n",
    "        pred_array_cells = np.zeros((A_pinv.shape[0],3,len(bins)))# 0: mean, 1: std, 2: bins\n",
    "\n",
    "        for i_bin in range(len(bins)):\n",
    "            tmp_B = bins[i_bin]\n",
    "            tmp_mean = intercepts_B_vs_mean + slopes_B_vs_mean * tmp_B\n",
    "            tmp_mean = tmp_mean.transpose()\n",
    "\n",
    "            #cal for beads\n",
    "            tmp_cov_beads = intercepts_B_vs_cov_beads + slopes_B_vs_cov_beads * tmp_B\n",
    "            tmp_cov_beads = tmp_cov_beads[:,:,0]\n",
    "\n",
    "            par_raw = np.empty((A_pinv.shape[0],3))# 0: mean, 1: std, 2: bins\n",
    "            for i in range(A_pinv.shape[0]):\n",
    "                A_pinv_oneline = A_pinv[i:(i+1),:]\n",
    "                u_N, sigma_N = SumNormal(u=tmp_mean, Z=tmp_cov_beads, weight=A_pinv_oneline)\n",
    "                if sigma_N < 0:\n",
    "                    sigma_N = - sigma_N\n",
    "                sigma_N = math.sqrt(sigma_N)\n",
    "                par_raw[i,0] = u_N\n",
    "                par_raw[i,1] = sigma_N\n",
    "                par_raw[i,2] = tmp_B\n",
    "            pred_array_beads[:,:,i_bin] = par_raw\n",
    "\n",
    "            #cal for cells\n",
    "            tmp_cov_cells = intercepts_B_vs_cov_cells + slopes_B_vs_cov_cells * tmp_B\n",
    "            tmp_cov_cells = tmp_cov_cells[:,:,0]\n",
    "\n",
    "            par_raw = np.empty((A_pinv.shape[0],3))# 0: mean, 1: std, 2: bins\n",
    "            for i in range(A_pinv.shape[0]):\n",
    "                A_pinv_oneline = A_pinv[i:(i+1),:]\n",
    "                u_N, sigma_N = SumNormal(u=tmp_mean, Z=tmp_cov_cells, weight=A_pinv_oneline)\n",
    "                if sigma_N < 0:\n",
    "                    sigma_N = - sigma_N\n",
    "                sigma_N = math.sqrt(sigma_N)\n",
    "                par_raw[i,0] = u_N\n",
    "                par_raw[i,1] = sigma_N\n",
    "                par_raw[i,2] = tmp_B\n",
    "            pred_array_cells[:,:,i_bin] = par_raw\n",
    "\n",
    "\n",
    "        u_line_data = pred_array_beads[y_idx,0,:]\n",
    "        u_line_data = np.zeros((pred_array_beads[y_idx,0,:].shape)) #set mean to be 0\n",
    "        sigma_data_beads = pred_array_beads[y_idx,1,:]\n",
    "        lower_line_data_beads = u_line_data - 1.96 * sigma_data_beads\n",
    "        higher_line_data_beads = u_line_data + 1.96 * sigma_data_beads\n",
    "\n",
    "        sigma_data_cells = pred_array_cells[y_idx,1,:]\n",
    "        lower_line_data_cells = u_line_data - 1.96 * sigma_data_cells\n",
    "        higher_line_data_cells = u_line_data + 1.96 * sigma_data_cells\n",
    "\n",
    "        #plot\n",
    "\n",
    "        coverage_rate_beads, covered_count_beads, total_count_beads = calculate_coverage_rate(sampled_x, sampled_y, bins, \n",
    "                                                                            lower_line_data_beads, higher_line_data_beads)\n",
    "        coverage_rate_cells, covered_count_cells, total_count_cells = calculate_coverage_rate(sampled_x, sampled_y, bins, \n",
    "                                                                            lower_line_data_cells, higher_line_data_cells)    \n",
    "        coverage_results_beads_uncorrected.append({\n",
    "                'Dim': A_df.columns[y_idx],\n",
    "                'Coverage Rate': round(coverage_rate_beads, 4),\n",
    "                'Covered Count': covered_count_beads,\n",
    "                'Total Count': total_count_beads\n",
    "            })\n",
    "        coverage_results_cells_uncorrected.append({\n",
    "                'Dim': A_df.columns[y_idx],\n",
    "                'Coverage Rate': round(coverage_rate_cells, 4),\n",
    "                'Covered Count': covered_count_cells,\n",
    "                'Total Count': total_count_cells\n",
    "            })\n",
    "        title_text = (f\"Scatter Plot of Unmixed Data (panel: 31 fluors + AF; corrected compensated result)\\n\"\n",
    "                    f\"(Coverage_beads: {coverage_rate_beads*100:.2f}%, {covered_count_beads}/{total_count_beads})\\n\"\n",
    "                    f\"(Coverage_cells: {coverage_rate_cells*100:.2f}%, {covered_count_cells}/{total_count_cells})\\n\")\n",
    "        plot_publication_scatter(\n",
    "            sampled_x, sampled_y,\n",
    "            bins, u_line_data,\n",
    "            lower_line_data_beads, higher_line_data_beads,\n",
    "            lower_line_data_cells, higher_line_data_cells,\n",
    "            xlabel=A_df.columns[x_idx],\n",
    "            ylabel=A_df.columns[y_idx],\n",
    "            title_text=title_text,\n",
    "            save_path=Output_dir+\"/fitting_scatter_uncorrected_\"+A_df.columns[y_idx]+\".pdf\"\n",
    "        )\n",
    "\n",
    "        #plot\n",
    "        coverage_rate_beads, covered_count_beads, total_count_beads = calculate_coverage_rate(sampled_x_corrected, sampled_y_corrected, bins, \n",
    "                                                                            lower_line_data_beads, higher_line_data_beads)\n",
    "        coverage_rate_cells, covered_count_cells, total_count_cells = calculate_coverage_rate(sampled_x_corrected, sampled_y_corrected, bins, \n",
    "                                                                            lower_line_data_cells, higher_line_data_cells)    \n",
    "        coverage_results_beads_corrected.append({\n",
    "                'Dim': A_df.columns[y_idx],\n",
    "                'Coverage Rate': round(coverage_rate_beads, 4),\n",
    "                'Covered Count': covered_count_beads,\n",
    "                'Total Count': total_count_beads\n",
    "            })\n",
    "        coverage_results_cells_corrected.append({\n",
    "                'Dim': A_df.columns[y_idx],\n",
    "                'Coverage Rate': round(coverage_rate_cells, 4),\n",
    "                'Covered Count': covered_count_cells,\n",
    "                'Total Count': total_count_cells\n",
    "            })\n",
    "        title_text = (f\"Scatter Plot of Unmixed Data (panel: 31 fluors + AF; corrected compensated result)\\n\"\n",
    "                    f\"(Coverage_beads: {coverage_rate_beads*100:.2f}%, {covered_count_beads}/{total_count_beads})\\n\"\n",
    "                    f\"(Coverage_cells: {coverage_rate_cells*100:.2f}%, {covered_count_cells}/{total_count_cells})\\n\")\n",
    "        plot_publication_scatter(\n",
    "            sampled_x_corrected, sampled_y_corrected,\n",
    "            bins, u_line_data,\n",
    "            lower_line_data_beads, higher_line_data_beads,\n",
    "            lower_line_data_cells, higher_line_data_cells,\n",
    "            xlabel=A_df.columns[x_idx],\n",
    "            ylabel=A_df.columns[y_idx],\n",
    "            title_text=title_text,\n",
    "            save_path=Output_dir+\"/fitting_scatter_corrected_\"+A_df.columns[y_idx]+\".pdf\"\n",
    "        )\n",
    "\n",
    "    coverage_table_beads_corrected = pd.DataFrame(coverage_results_beads_corrected)\n",
    "    coverage_table_beads_corrected.to_csv(Output_dir+\"/coverage_beads_corrected_summary.csv\", index=False)\n",
    "    coverage_table_cells_corrected = pd.DataFrame(coverage_results_cells_corrected)\n",
    "    coverage_table_cells_corrected.to_csv(Output_dir+\"/coverage_cells_corrected_summary.csv\", index=False)\n",
    "    coverage_table_beads_uncorrected = pd.DataFrame(coverage_results_beads_uncorrected)\n",
    "    coverage_table_beads_uncorrected.to_csv(Output_dir+\"/coverage_beads_uncorrected_summary.csv\", index=False)\n",
    "    coverage_table_cells_uncorrected = pd.DataFrame(coverage_results_cells_uncorrected)\n",
    "    coverage_table_cells_uncorrected.to_csv(Output_dir+\"/coverage_cells_uncorrected_summary.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f53f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "residualModel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
