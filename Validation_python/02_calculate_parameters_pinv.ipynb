{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "\n",
    "import copy\n",
    "import textwrap\n",
    "\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_mtx_corr_heatmap_with_pval(matrix_a, matrix_b, method='pearson', figsize=(20, 10), title=None, \n",
    "                                   xtitle = None, ytitle = None, save_addr=None):\n",
    "    \"\"\"\n",
    "    matrix_a: First DataFrame, columns are variables, rows are observations\n",
    "    matrix_b: Second DataFrame, columns are variables, rows are observations, must have the same shape as matrix_a\n",
    "    method: Correlation method, options are 'pearson', 'spearman', 'kendall'\n",
    "    figsize: Size of the figure\n",
    "    title: Title of the plot\n",
    "    save_addr: Path to save the plot (PDF format)\n",
    "    \"\"\"\n",
    "    assert matrix_a.shape == matrix_b.shape, \"The two matrices must have the same shape\"\n",
    "\n",
    "    matrix_a = pd.DataFrame(matrix_a, columns = matrix_b.columns)\n",
    "    corr = pd.DataFrame(np.zeros((matrix_a.shape[1], matrix_b.shape[1])), columns=matrix_b.columns, index=matrix_a.columns)\n",
    "    p_values = pd.DataFrame(np.ones((matrix_a.shape[1], matrix_b.shape[1])), columns=matrix_b.columns, index=matrix_a.columns)\n",
    "\n",
    "    for i in range(matrix_a.shape[1]):\n",
    "        for j in range(matrix_b.shape[1]):\n",
    "            x = matrix_a.iloc[:, i]\n",
    "            y = matrix_b.iloc[:, j]\n",
    "            mask = ~np.logical_or(np.isnan(x), np.isnan(y))\n",
    "            if np.sum(mask) > 0:\n",
    "                if method == 'pearson':\n",
    "                    corr.iloc[i, j], p_values.iloc[i, j] = pearsonr(x[mask], y[mask])\n",
    "                elif method == 'kendall':\n",
    "                    corr.iloc[i, j], p_values.iloc[i, j] = kendalltau(x[mask], y[mask])\n",
    "                elif method == 'spearman':\n",
    "                    corr.iloc[i, j], p_values.iloc[i, j] = spearmanr(x[mask], y[mask])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.title(title, fontsize=20)\n",
    "\n",
    "    heatmap = sns.heatmap(corr,\n",
    "                          annot=True,\n",
    "                          annot_kws={\"fontsize\": 10},\n",
    "                          fmt='.2f',\n",
    "                          linewidths=0.5,\n",
    "                          cmap='coolwarm',\n",
    "                          ax=ax,\n",
    "                          cbar=True,\n",
    "                          cbar_kws={'shrink': 0.3, 'aspect': 30})\n",
    "\n",
    "    cbar = heatmap.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "    cbar.set_label('Correlation R value', fontsize=12)\n",
    "\n",
    "    max_corr = np.max(corr.max())\n",
    "    min_corr = np.min(corr.min())\n",
    "\n",
    "    for i in range(p_values.shape[0]):\n",
    "        for j in range(p_values.shape[1]):\n",
    "            p_value = p_values.iloc[i, j]\n",
    "            if not np.isnan(p_value):\n",
    "                correlation_value = corr.iloc[i, j]\n",
    "                text_color = 'white' if correlation_value >= (max_corr - 0.4) or correlation_value <= (min_corr + 0.4) else 'black'\n",
    "                if p_value <= 0.01:\n",
    "                    ax.text(j + 0.5, i + 0.8, '**', ha='center', va='center', fontsize=8, color=text_color)\n",
    "                elif p_value <= 0.05:\n",
    "                    ax.text(j + 0.5, i + 0.8, '*', ha='center', va='center', fontsize=8, color=text_color)\n",
    "\n",
    "    ax.set_xticklabels([textwrap.fill(label.get_text(), 12) for label in ax.get_xticklabels()], rotation=90, ha=\"center\")\n",
    "    ax.set_yticklabels([textwrap.fill(label.get_text(), 12) for label in ax.get_yticklabels()], rotation=0, ha=\"right\")\n",
    "\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_fontsize(12)\n",
    "    for label in ax.get_yticklabels():\n",
    "        label.set_fontsize(12)\n",
    "\n",
    "    ax.set_xlabel(xtitle, fontsize=15)\n",
    "    ax.set_ylabel(ytitle, fontsize=15)\n",
    "\n",
    "    ax.grid(False)\n",
    "    plt.figtext(0.1, 0.05, '**: p<=0.01; *: p<=0.05.', fontsize=15)\n",
    "\n",
    "    if save_addr:\n",
    "        plt.savefig(save_addr, format='pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_heatmap_with_pval(df, method = 'pearson', figsize=(20, 10), title=None, xtitle = None, ytitle = None, save_addr = None):\n",
    "  \"\"\"\n",
    "  df: dataframe to be used. Ensured the dataframe has been sliced to contain only the column you need. It accepts only numerical columns\n",
    "  method: default uses the pearson method. It overall permits 3 methods; 'pearson', 'spearman' and 'kendall'\n",
    "  figsize: default is (20, 10) but you can change it based on your preference\n",
    "  title: Specify the title for your chart, default is None\n",
    "  \"\"\"\n",
    "  # Make a copy of the df\n",
    "  data = df.copy()\n",
    "  # Check features correlation\n",
    "  corr = data.corr(method = method)\n",
    "\n",
    "  # Create a mask to hide the upper triangle\n",
    "  mask = np.zeros_like(corr, dtype=bool)\n",
    "  mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "  # Set the diagonal elements of the mask to False to display self-correlation\n",
    "  np.fill_diagonal(mask, False)\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=figsize)\n",
    "  plt.title(title, fontsize=20)\n",
    "\n",
    "  # Create the heatmap with the custom mask\n",
    "  heatmap = sns.heatmap(corr,\n",
    "                        annot=True,\n",
    "                        annot_kws={\"fontsize\": 10},  # Adjust annotation font size\n",
    "                        fmt='.2f',\n",
    "                        linewidths=0.5,\n",
    "                        cmap='coolwarm',\n",
    "                        mask=mask,\n",
    "                        ax=ax,\n",
    "                        vmax=1,\n",
    "                        vmin=-1,\n",
    "                        cbar=True,\n",
    "                        cbar_kws={'shrink': 0.3, 'aspect': 30})\n",
    "    \n",
    "  cbar = heatmap.collections[0].colorbar\n",
    "  cbar.ax.tick_params(labelsize=12)\n",
    "  cbar.set_label('Correlation R valUe', fontsize=12)\n",
    "\n",
    "  # Create a function to calculate and format p-values\n",
    "  p_values = np.full((corr.shape[0], corr.shape[1]), np.nan)\n",
    "  for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[1]):\n",
    "      x = data.iloc[:, i]\n",
    "      y = data.iloc[:, j]\n",
    "      mask = ~np.logical_or(np.isnan(x), np.isnan(y))\n",
    "      if np.sum(mask) > 0:\n",
    "        if method == 'pearson':\n",
    "          p_values[i, j] = pearsonr(x[mask], y[mask])[1] #Changes based on the method chosen in the function\n",
    "        elif method == 'kendall':\n",
    "          p_values[i, j] = kendalltau(x[mask], y[mask])[1]\n",
    "        elif method == 'spearman':\n",
    "          p_values[i, j] = spearmanr(x[mask], y[mask])[1]\n",
    "  \n",
    "  p_values = pd.DataFrame(p_values, columns=corr.columns, index=corr.index)\n",
    "\n",
    "  # Create a mask for the p-values heatmap\n",
    "  mask_pvalues = np.triu(np.ones_like(p_values), k=1)\n",
    "\n",
    "  # Calculate the highest and lowest correlation coefficients\n",
    "  max_corr = np.max(corr.max())\n",
    "  min_corr = np.min(corr.min())\n",
    "  \n",
    "  # Annotate the heatmap with p-values and change text color based on correlation value\n",
    "  for i in range(p_values.shape[0]):\n",
    "    for j in range(p_values.shape[1]):\n",
    "      if mask_pvalues[i, j]:\n",
    "        p_value = p_values.iloc[i, j]\n",
    "        if not np.isnan(p_value):\n",
    "          correlation_value = corr.iloc[i, j]\n",
    "          text_color = 'white' if correlation_value >= (max_corr - 0.4) or correlation_value <= (min_corr + 0.4) else 'black'\n",
    "          if p_value <= 0.01:\n",
    "            #include double asterisks for p-value <= 0.01\n",
    "            ax.text(i + 0.5, j + 0.8, f'**',\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=8,\n",
    "                    color=text_color)\n",
    "          elif p_value <= 0.05:\n",
    "            #include single asterisks for p-value <= 0.05\n",
    "            ax.text(i + 0.5, j + 0.8, f'*',\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=8,\n",
    "                    color=text_color)\n",
    "          else:\n",
    "            ax.text(i + 0.5, j + 0.8, f'',\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=8,\n",
    "                    color=text_color)\n",
    "\n",
    "  # Customize x-axis labels\n",
    "  x_labels = [textwrap.fill(label.get_text(), 12) for label in ax.get_xticklabels()]\n",
    "  ax.set_xticklabels(x_labels, rotation=90, ha=\"center\")\n",
    "\n",
    "  # Customize y-axis labels\n",
    "  y_labels = [textwrap.fill(label.get_text(), 12) for label in ax.get_yticklabels()]\n",
    "  ax.set_yticklabels(y_labels, rotation=0, ha=\"right\")\n",
    "\n",
    "  for label in ax.get_xticklabels():\n",
    "    label.set_fontsize(12)\n",
    "\n",
    "  for label in ax.get_yticklabels():\n",
    "      label.set_fontsize(12)\n",
    "  \n",
    "  ax.set_xlabel(xtitle, fontsize=15)\n",
    "  ax.set_ylabel(ytitle, fontsize=15)\n",
    "\n",
    "  ax.grid(False)\n",
    "  # Add a footnote below and to the right side of the chart\n",
    "  plt.figtext(0.1,0.05,\n",
    "              '**: p<=0.01; *: p<=0.05.',\n",
    "              fontsize=15)\n",
    "  plt.savefig(save_addr, format='pdf')\n",
    "  plt.close()\n",
    "  #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pack(file,residual_mtx,data, selected_channels, Output_dir,fit_method = \"norm\", B = None):\n",
    "\n",
    "    #E1: distribution estimation and hist plots\n",
    "    \n",
    "\n",
    "    col_num = 6\n",
    "    row_num = math.ceil(len(selected_channels) / col_num)\n",
    "    fig, axs = plt.subplots(row_num, col_num, figsize=(24,4*row_num))\n",
    "    fig_count = 0\n",
    "    n_row = 0\n",
    "    n_col = 0\n",
    "\n",
    "    if (fit_method == \"norm\"):\n",
    "        est_pd = pd.DataFrame(columns=data.columns,index=['mean','std'])\n",
    "        for i in range(data.shape[1]):\n",
    "            x = residual_mtx[:,i]\n",
    "            \n",
    "            threshold = np.percentile(x, 90)\n",
    "            x = x[x <= threshold]\n",
    "\n",
    "            mean, std = norm.fit(x)\n",
    "            est_pd.iloc[0,i] = mean\n",
    "            est_pd.iloc[1,i] = std\n",
    "\n",
    "            if n_col >= col_num:\n",
    "                n_col = 0\n",
    "                n_row += 1\n",
    "            \n",
    "            axs[n_row,n_col].hist(x, bins=60, density=True, alpha=0.6, color='skyblue', edgecolor='black')\n",
    "            xmin, xmax = axs[n_row,n_col].get_xlim()\n",
    "            x_fit = np.linspace(xmin, xmax, 100)\n",
    "            p_fit = norm.pdf(x_fit, mean, std)\n",
    "            axs[n_row,n_col].plot(x_fit, p_fit, 'r', linewidth=2)\n",
    "\n",
    "            axs[n_row,n_col].set_title(str(data.columns[i])+\" (\"+fit_method+\")\")\n",
    "            axs[n_row,n_col].set_xlabel(\"Signal\")\n",
    "            axs[n_row,n_col].set_ylabel(\"Prob\")\n",
    "            axs[n_row,n_col].grid(True)\n",
    "\n",
    "            n_col += 1\n",
    "    elif (fit_method == \"poisson\"):\n",
    "        est_pd = pd.DataFrame(columns=data.columns, index=['lambda'])\n",
    "        for i in range(data.shape[1]):\n",
    "            x = residual_mtx[:,i]\n",
    "            lambda_hat = np.mean(x)\n",
    "            est_pd.iloc[0,i] = lambda_hat\n",
    "\n",
    "            if n_col >= col_num:\n",
    "                n_col = 0\n",
    "                n_row += 1\n",
    "            \n",
    "            axs[n_row,n_col].hist(x, bins=30, density=True, alpha=0.6, color='skyblue', edgecolor='black')\n",
    "            xmin, xmax = axs[n_row,n_col].get_xlim()\n",
    "            x_fit = np.arange(int(xmin), int(xmax))\n",
    "            p_fit = poisson.pmf(x_fit, lambda_hat)\n",
    "            axs[n_row,n_col].plot(x_fit, p_fit, 'r', linewidth=2)\n",
    "\n",
    "            axs[n_row,n_col].set_title(str(data.columns[i])+\" (\"+fit_method+\")\")\n",
    "            axs[n_row,n_col].set_xlabel(\"Signal\")\n",
    "            axs[n_row,n_col].set_ylabel(\"Prob\")\n",
    "            axs[n_row,n_col].grid(True)\n",
    "\n",
    "            n_col += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Output_dir+\"/residual_distribution_across_channels_\"+file+\".pdf\", format='pdf')  \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #E1: plot row lineplots of residual\n",
    "    data_for_line = copy.deepcopy(residual_mtx)\n",
    "    data_for_line = pd.DataFrame(data_for_line,columns=selected_channels)\n",
    "    data_for_line =data_for_line.transpose()\n",
    "    data_for_line.insert(0, \"channel\", pd.Series(selected_channels.tolist(),index=selected_channels))\n",
    "\n",
    "    columns = data_for_line.columns\n",
    "    title = file\n",
    "    # set figure size\n",
    "    my_dpi=96\n",
    "    plt.figure(figsize=(2500/my_dpi, 1500/my_dpi), dpi=my_dpi)\n",
    "\n",
    "    for column in columns[1:]:\n",
    "        plt.plot(data_for_line[column], label = str(column))\n",
    "    plt.title(title, loc='left', fontsize=12, fontweight=0)\n",
    "    plt.xticks(rotation = 90,fontsize = 10)\n",
    "    plt.xlabel(\"Channels\",fontsize = 10)\n",
    "    plt.ylabel(\"Signal\",fontsize = 10)\n",
    "    plt.ylim(-100,100)\n",
    "    #plt.show()\n",
    "\n",
    "    plt.savefig(Output_dir+\"/lineplot_residual_\"+title+\".pdf\", format='pdf')  \n",
    "    plt.close()\n",
    "\n",
    "    #E1: plot correlation between residuals of channels\n",
    "    data_for_simi = copy.deepcopy(residual_mtx)\n",
    "    corr_heatmap_with_pval(pd.DataFrame(data_for_simi,columns=selected_channels),figsize=(40,30),\n",
    "                           title='Correlation matrix between residuals of channels',\n",
    "                           xtitle = \"Residual\", ytitle = \"Residual\",\n",
    "                        save_addr = Output_dir+\"/cor_matrix_residual_vs_res_\"+title+\".pdf\")\n",
    "                        \n",
    "    #E1:\n",
    "    two_mtx_corr_heatmap_with_pval(matrix_a=residual_mtx, matrix_b = data, figsize=(40, 30), \n",
    "                                   title='Correlation matrix between residuals and raw of channels', \n",
    "                                   xtitle = \"Residual\", ytitle = \"Raw\",\n",
    "                                   save_addr = Output_dir+\"/cor_matrix_residual_vs_raw_\"+title+\".pdf\")\n",
    "\n",
    "    #E1: plot similarity matrix heatmap between residuals of channels (redundant)\n",
    "    title = file\n",
    "\n",
    "    data_for_simi = copy.deepcopy(residual_mtx)\n",
    "    cos_sim = cosine_similarity(data_for_simi.transpose())\n",
    "\n",
    "    df = pd.DataFrame(cos_sim,index=selected_channels,columns=selected_channels)\n",
    "        \n",
    "    #plt.figure(figsize=(fig_weith, fig_height))\n",
    "    sns.set_theme(style=\"ticks\",font_scale=0.5)\n",
    "    p = sns.clustermap(df,cmap=sns.color_palette(\"blend:#ffffff,#365E32\", as_cmap=True), annot=True, fmt=\".2f\",cbar_pos=[0.04, 0.04, 0.02, 0.1],figsize=(16, 16))\n",
    "    p.ax_cbar.set_title('Cosine Similarity')\n",
    "    p.ax_heatmap.set_xlabel('Channels')\n",
    "    p.ax_heatmap.set_ylabel('Channels')\n",
    "    p.ax_heatmap.set_title('Similarity matrix for signal deviation \\n Signal-median(Signal) \\n '+ title, weight=\"bold\", fontsize=10) \n",
    "    p.ax_col_dendrogram.set_visible(False)\n",
    "    plt.savefig(Output_dir+\"/cos_matrix_residual_\"+title+\".pdf\", format='pdf')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    #E1: scatter plot between B and channel residuals\n",
    "    if B is not None:\n",
    "        col_num = 6\n",
    "        row_num = math.ceil(len(selected_channels) / col_num)\n",
    "\n",
    "        plot_ptx = residual_mtx\n",
    "\n",
    "        threshold = np.percentile(np.abs(B), 90)\n",
    "        mask = np.abs(B) <= threshold # shape: (1, 1000)\n",
    "\n",
    "\n",
    "        fig, axs = plt.subplots(row_num, col_num, figsize=(24, 4*row_num))\n",
    "\n",
    "        for i in range(plot_ptx.shape[1]):\n",
    "            row = i // col_num\n",
    "            col = i % col_num\n",
    "            \n",
    "            x_data = B[mask].flatten()\n",
    "            y_data = plot_ptx[mask.flatten(), i]\n",
    "            \n",
    "            R, P = pearsonr(x_data, y_data)\n",
    "            significance = \"**\" if P < 0.01 else \"*\" if P < 0.05 else \"\"\n",
    "\n",
    "            axs[row, col].scatter(x_data, y_data, s=10, alpha=0.7)\n",
    "            axs[row, col].set_title(f\"{selected_channels[i]} (R={R:.2f}, P={P:.2e}{significance})\")\n",
    "            axs[row, col].set_xlabel(\"Coef of fluor (count)\")\n",
    "            axs[row, col].set_ylabel(\"Residual of \" + selected_channels[i])\n",
    "            #axs[row, col].set_xscale('symlog')\n",
    "            #axs[row, col].set_yscale('symlog')\n",
    "            \n",
    "            model = LinearRegression().fit(x_data.reshape(-1, 1), y_data)\n",
    "            y_fit = model.predict(x_data.reshape(-1, 1))\n",
    "            axs[row, col].plot(x_data, y_fit, color='red', linewidth=1)\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(Output_dir+\"/scatter_B_vs_residuals_\"+file+\".pdf\", format='pdf')  \n",
    "        plt.close()\n",
    "\n",
    "    #E1: scatter plot between B and channel raw\n",
    "    if B is not None:\n",
    "        col_num = 6\n",
    "        row_num = math.ceil(len(selected_channels) / col_num)\n",
    "\n",
    "        plot_ptx = np.array(data)\n",
    "\n",
    "        threshold = np.percentile(np.abs(B), 90)\n",
    "        mask = np.abs(B) <= threshold # shape: (1, 1000)\n",
    "\n",
    "\n",
    "        fig, axs = plt.subplots(row_num, col_num, figsize=(24, 4*row_num))\n",
    "\n",
    "        for i in range(plot_ptx.shape[1]):\n",
    "            row = i // col_num\n",
    "            col = i % col_num\n",
    "            \n",
    "            x_data = B[mask].flatten()\n",
    "            y_data = plot_ptx[mask.flatten(), i]\n",
    "            \n",
    "            R, P = pearsonr(x_data, y_data)\n",
    "            significance = \"**\" if P < 0.01 else \"*\" if P < 0.05 else \"\"\n",
    "\n",
    "            axs[row, col].scatter(x_data, y_data, s=10, alpha=0.7)\n",
    "            axs[row, col].set_title(f\"{selected_channels[i]} (R={R:.2f}, P={P:.2e}{significance})\")\n",
    "            axs[row, col].set_xlabel(\"Coef of fluor (count)\")\n",
    "            axs[row, col].set_ylabel(\"Residual of \" + selected_channels[i])\n",
    "            #axs[row, col].set_xscale('symlog')\n",
    "            #axs[row, col].set_yscale('symlog')\n",
    "            \n",
    "            model = LinearRegression().fit(x_data.reshape(-1, 1), y_data)\n",
    "            y_fit = model.predict(x_data.reshape(-1, 1))\n",
    "            axs[row, col].plot(x_data, y_fit, color='red', linewidth=1)\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(Output_dir+\"/scatter_B_vs_raw_\"+file+\".pdf\", format='pdf')  \n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    return est_pd\n",
    "\n",
    "def calculate_sigma_Y(sigma_Z, sigma_X):\n",
    "    if sigma_Z < sigma_X:\n",
    "        return 0\n",
    "    else:\n",
    "        sigma_Y = math.sqrt(sigma_Z**2 - sigma_X**2)\n",
    "        return sigma_Y\n",
    "\n",
    "def mkdir_silent(directory_name):\n",
    "    try:\n",
    "        os.mkdir(directory_name)\n",
    "        print(f\"Directory '{directory_name}' created successfully.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Directory '{directory_name}' already exists.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def signature_lineplot(sig,sig_name,Output_dir):\n",
    "    \n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(8)\n",
    "    f.set_figheight(4)\n",
    "    plt.plot(sig)\n",
    "    plt.title(sig_name)\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    \n",
    "    plt.savefig(Output_dir+\"/Normalized_Sig_lineplot_\"+sig_name+\".pdf\", format='pdf')  \n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def B_bin_plot_pack(file, B, residual_mtx, selected_channels, Output_dir, bin_num=20,count_thre = 20,\n",
    "                    filter_low=1,filter_high=99.9,filter_aroundzero=0.2, do_plot = False):\n",
    "\n",
    "    # step 1 remove outlier \n",
    "    print(\"residual_mtx\")\n",
    "    print(residual_mtx.shape)\n",
    "    # remove higher outlier\n",
    "    threshold_high = np.percentile(B, filter_high)\n",
    "    mask_high = B < threshold_high\n",
    "    B_filtered_high = B[mask_high]\n",
    "    rows_to_keep_high = np.where(mask_high[0])[0]\n",
    "    residual_mtx_filtered_high = residual_mtx[rows_to_keep_high, :]\n",
    "    print(\"residual_mtx_filtered_high\")\n",
    "    print(residual_mtx_filtered_high.shape)\n",
    "    # remove lower outlier\n",
    "    threshold_low = np.percentile(B, filter_low)\n",
    "    mask_low = B_filtered_high > threshold_low\n",
    "    B_filtered_low = B_filtered_high[mask_low]\n",
    "    rows_to_keep_low = np.where(mask_low)[0]\n",
    "    residual_mtx_filtered_low = residual_mtx_filtered_high[rows_to_keep_low, :]\n",
    "    print(\"residual_mtx_filtered_low\")\n",
    "    print(residual_mtx_filtered_low.shape)\n",
    "    #remove around zero value\n",
    "    threshold_aroundzero = (1 - filter_aroundzero)* B_filtered_low.min() + filter_aroundzero * B_filtered_low.max()\n",
    "    mask_aroundzero = B_filtered_low > threshold_aroundzero\n",
    "    B_filtered = B_filtered_low[mask_aroundzero]\n",
    "    rows_to_keep_aroundzero = np.where(mask_aroundzero)[0]\n",
    "    residual_mtx_filtered = residual_mtx_filtered_low[rows_to_keep_aroundzero, :]\n",
    "    print(\"residual_mtx_filtered\")\n",
    "    print(residual_mtx_filtered.shape)\n",
    "    # step 2 calcualte bin size \n",
    "    bin_size = (B_filtered.max() - B_filtered.min()) / bin_num\n",
    "\n",
    "    # step 3 initail save matrix\n",
    "    mean_std_matrix = np.zeros((3, len(selected_channels), bin_num))  # 0: mean, 1: std, 2: bin midpoint\n",
    "    cov_matrices = np.zeros((len(selected_channels), len(selected_channels), bin_num)) \n",
    "\n",
    "\n",
    "    \n",
    "    # step 4 loop for bins\n",
    "    for i in range(bin_num):\n",
    "        bin_min = B_filtered.min() + i * bin_size - 1 * bin_size\n",
    "        bin_max = bin_min + bin_size + 1 * bin_size\n",
    "        bin_mid = (bin_min + bin_max) / 2\n",
    "\n",
    "        bin_mask = (B_filtered >= bin_min) & (B_filtered < bin_max)\n",
    "        bin_points = B_filtered[bin_mask]\n",
    "        #print(f\"point{i}: {bin_points.size}\")\n",
    "\n",
    "        if bin_points.size > count_thre:\n",
    "            bin_indices = np.where(bin_mask)[0]\n",
    "            residuals_in_bin = residual_mtx_filtered[bin_indices, :]\n",
    "\n",
    "            means = []\n",
    "            stds = []\n",
    "            for col in range(residuals_in_bin.shape[1]):\n",
    "                mean, std = robust_stats_zscore(residuals_in_bin[:, col])\n",
    "                means.append(mean)\n",
    "                stds.append(std)\n",
    "            means = np.array(means)\n",
    "            stds = np.array(stds)\n",
    "\n",
    "            #print(f\"stds[5]: {stds[5]}\")\n",
    "            mean_std_matrix[0, :, i] = means\n",
    "            mean_std_matrix[1, :, i] = stds\n",
    "\n",
    "            cov_matrix = robust_covariance_matrix(residuals_in_bin)\n",
    "            cov_matrices[:, :, i] = cov_matrix\n",
    "\n",
    "        mean_std_matrix[2, :, i] = bin_mid\n",
    "\n",
    "\n",
    "    # step 5 scatter plot B vs mean\n",
    "    col_num = 6\n",
    "    row_num = math.ceil(len(selected_channels) / col_num)\n",
    "\n",
    "    \n",
    "    intercepts_B_vs_mean = np.zeros((1, len(selected_channels)))\n",
    "    slopes_B_vs_mean = np.zeros((1, len(selected_channels)))\n",
    "\n",
    "    fig, axs = plt.subplots(row_num, col_num, figsize=(24, 4*row_num))\n",
    "\n",
    "    for i in range(mean_std_matrix.shape[1]):\n",
    "        row = i // col_num\n",
    "        col = i % col_num\n",
    "\n",
    "        x_data = mean_std_matrix[2, i, :]#bin\n",
    "        y_data = mean_std_matrix[0, i, :]#mean\n",
    "\n",
    "        valid_mask = (mean_std_matrix[0, i, :] != 0) & (mean_std_matrix[1, i, :] != 0)#mean and std != 0\n",
    "        x_data = x_data[valid_mask]\n",
    "        y_data = y_data[valid_mask]\n",
    "\n",
    "        if x_data.size > 3 and y_data.size > 3:\n",
    "            R, P = pearsonr(x_data, y_data)\n",
    "            significance = \"**\" if P < 0.01 else \"*\" if P < 0.05 else \"\"\n",
    "\n",
    "            axs[row, col].scatter(x_data, y_data, s=10, alpha=0.7)\n",
    "            axs[row, col].set_title(f\"{selected_channels[i]} (R={R:.2f}, P={P:.2e}{significance})\")\n",
    "            axs[row, col].set_xlabel(\"B (bin midpoint)\")\n",
    "            axs[row, col].set_ylabel(\"Mean of \" + selected_channels[i])\n",
    "\n",
    "            #model = LinearRegression().fit(x_data.reshape(-1, 1), y_data)\n",
    "            model = TheilSenRegressor().fit(x_data.reshape(-1, 1), y_data)\n",
    "\n",
    "            y_fit = model.predict(x_data.reshape(-1, 1))\n",
    "            axs[row, col].plot(x_data, y_fit, color='red', linewidth=1)\n",
    "\n",
    "            intercepts_B_vs_mean[0, i] = model.intercept_\n",
    "            slopes_B_vs_mean[0, i] = model.coef_[0]\n",
    "    #if do_plot:\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Output_dir+\"/scatter_across_channels_Bbins_vs_mean_\"+file+\".pdf\", format='pdf')\n",
    "    plt.close()\n",
    "\n",
    "    # step 6 scatter plot B vs std\n",
    "    col_num = 6\n",
    "    row_num = math.ceil(len(selected_channels) / col_num)\n",
    "\n",
    "    intercepts_B_vs_std = np.zeros((1, len(selected_channels)))\n",
    "    slopes_B_vs_std = np.zeros((1, len(selected_channels)))\n",
    "\n",
    "    fig, axs = plt.subplots(row_num, col_num, figsize=(24, 4*row_num))\n",
    "\n",
    "    for i in range(mean_std_matrix.shape[1]):\n",
    "        row = i // col_num\n",
    "        col = i % col_num\n",
    "\n",
    "        x_data = mean_std_matrix[2, i, :]#bin\n",
    "        y_data = mean_std_matrix[1, i, :]#std\n",
    "\n",
    "        valid_mask = (mean_std_matrix[0, i, :] != 0) & (mean_std_matrix[1, i, :] != 0)#mean and std != 0\n",
    "        x_data = x_data[valid_mask]\n",
    "        y_data = y_data[valid_mask]\n",
    "\n",
    "        if x_data.size > 3 and y_data.size > 3:\n",
    "            R, P = pearsonr(x_data, y_data)\n",
    "            significance = \"**\" if P < 0.01 else \"*\" if P < 0.05 else \"\"\n",
    "\n",
    "            axs[row, col].scatter(x_data, y_data, s=10, alpha=0.7)\n",
    "            axs[row, col].set_title(f\"{selected_channels[i]} (R={R:.2f}, P={P:.2e}{significance})\")\n",
    "            axs[row, col].set_xlabel(\"B (bin midpoint)\")\n",
    "            axs[row, col].set_ylabel(\"Std of \" + selected_channels[i])\n",
    "\n",
    "            #model = LinearRegression().fit(x_data.reshape(-1, 1), y_data)\n",
    "            model = TheilSenRegressor().fit(x_data.reshape(-1, 1), y_data)\n",
    "            y_fit = model.predict(x_data.reshape(-1, 1))\n",
    "            axs[row, col].plot(x_data, y_fit, color='red', linewidth=1)\n",
    "\n",
    "            intercepts_B_vs_std[0, i] = model.intercept_\n",
    "            slopes_B_vs_std[0, i] = model.coef_[0]\n",
    "\n",
    "    #if do_plot:\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Output_dir+\"/scatter_across_channels_Bbins_vs_std_\"+file+\".pdf\", format='pdf')\n",
    "    plt.close()\n",
    "\n",
    "    #step 7 scatter plot B vs cov\n",
    "    \n",
    "    intercepts_B_vs_cov = np.zeros((len(selected_channels), len(selected_channels), 1))\n",
    "    slopes_B_vs_cov = np.zeros((len(selected_channels), len(selected_channels), 1))\n",
    "\n",
    "    for j in range(cov_matrices.shape[0]):#\n",
    "\n",
    "        col_num = 6\n",
    "        row_num = math.ceil(len(selected_channels) / col_num)\n",
    "        if do_plot:\n",
    "            fig, axs = plt.subplots(row_num, col_num, figsize=(24, 4*row_num))\n",
    "\n",
    "        for i in range(cov_matrices.shape[1]):\n",
    "            row = i // col_num\n",
    "            col = i % col_num\n",
    "\n",
    "            x_data = mean_std_matrix[2, i, :]#bin\n",
    "            y_data = cov_matrices[j, i, :]#cov\n",
    "\n",
    "            valid_mask = (mean_std_matrix[0, i, :] != 0) & (mean_std_matrix[1, i, :] != 0)#mean and std != 0\n",
    "            x_data = x_data[valid_mask]\n",
    "            y_data = y_data[valid_mask]\n",
    "\n",
    "            if x_data.size > 3 and y_data.size > 3:\n",
    "                R, P = pearsonr(x_data, y_data)\n",
    "                significance = \"**\" if P < 0.01 else \"*\" if P < 0.05 else \"\"\n",
    "\n",
    "                if do_plot:\n",
    "                    axs[row, col].scatter(x_data, y_data, s=10, alpha=0.7)\n",
    "                    axs[row, col].set_title(f\"Cov: {selected_channels[i]}\\n vs {selected_channels[j]} \\n(R={R:.2f}, P={P:.2e}{significance})\")\n",
    "\n",
    "                    axs[row, col].set_xlabel(\"B (bin midpoint)\")\n",
    "                    axs[row, col].set_ylabel(\"Cov\")\n",
    "\n",
    "                #model = LinearRegression().fit(x_data.reshape(-1, 1), y_data)\n",
    "                model = TheilSenRegressor().fit(x_data.reshape(-1, 1), y_data)\n",
    "                y_fit = model.predict(x_data.reshape(-1, 1))\n",
    "                if do_plot:\n",
    "                    axs[row, col].plot(x_data, y_fit, color='red', linewidth=1)\n",
    "\n",
    "                intercepts_B_vs_cov[j, i, 0] = model.intercept_\n",
    "                slopes_B_vs_cov[j, i, 0] = model.coef_[0]\n",
    "        \n",
    "        if do_plot:\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(Output_dir+\"/cov_\"+str(j)+\".pdf\", format='pdf')\n",
    "            plt.close()\n",
    "\n",
    "    \n",
    "    return mean_std_matrix, cov_matrices, intercepts_B_vs_mean, slopes_B_vs_mean, intercepts_B_vs_std, slopes_B_vs_std, intercepts_B_vs_cov, slopes_B_vs_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para_from_data(data):\n",
    "    \"\"\"\n",
    "    Compute the robust mean and standard deviation for each column of the data,\n",
    "    and return the covariance matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 2D array-like\n",
    "        Input two-dimensional numeric array, where each column represents a feature.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean_std_matrix : 2 x N array\n",
    "        The first row contains the means, and the second row contains the standard deviations.\n",
    "    cov_matrices : N x N array\n",
    "        The covariance matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.array(data)\n",
    "    num_features = data.shape[1]\n",
    "    mean_std_matrix = np.zeros((2, num_features))  # 0: mean, 1: std\n",
    "    for i in range(num_features):\n",
    "        mean, std = robust_stats_zscore(data[:, i], threshold=3.0)\n",
    "        mean_std_matrix[0, i] = mean\n",
    "        mean_std_matrix[1, i] = std\n",
    "\n",
    "    cov_matrix = robust_covariance_matrix(data, z_thresh=3.0)\n",
    "    return mean_std_matrix, cov_matrix\n",
    "\n",
    "\n",
    "def robust_stats_zscore(data, threshold=3.0, verbose=False):\n",
    "    \"\"\"\n",
    "    Identify and remove outliers using the Z-score method, then return the mean and\n",
    "    standard deviation of the non-outlier values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array-like\n",
    "        Input one-dimensional numeric array.\n",
    "    threshold : float\n",
    "        Z-score threshold; elements exceeding this value are considered outliers.\n",
    "    verbose : bool\n",
    "        Whether to print intermediate computation results.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean : float\n",
    "        Mean of the non-outlier values.\n",
    "    std : float\n",
    "        Standard deviation of the non-outlier values.\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.array(data)\n",
    "    z_scores = zscore(data)\n",
    "    mask = np.abs(z_scores) < threshold\n",
    "    filtered_data = data[mask]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Z-scores: {z_scores}\")\n",
    "        print(f\"outlier threshold: Â±{threshold}\")\n",
    "        print(f\"non-outlier data: {filtered_data}\")\n",
    "\n",
    "    mean = filtered_data.mean()\n",
    "    std = filtered_data.std()\n",
    "    return mean, std\n",
    "\n",
    "def robust_covariance_matrix(data, z_thresh=3.0):\n",
    "    \"\"\"\n",
    "    Compute the covariance matrix after removing outliers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray\n",
    "        Input 2D array with shape (n_samples, n_features).\n",
    "    z_thresh : float\n",
    "        Z-score threshold; elements exceeding this value are treated as outliers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cov_matrix : ndarray\n",
    "        Covariance matrix computed after excluding outliers, with shape\n",
    "        (n_features, n_features).\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.array(data)\n",
    "    n_samples, n_features = data.shape\n",
    "    cleaned_data = []\n",
    "\n",
    "    for col in range(n_features):\n",
    "        col_data = data[:, col]\n",
    "        z_scores = zscore(col_data)\n",
    "        mask = np.abs(z_scores) < z_thresh\n",
    "        # Replace outliers with NaN\n",
    "        cleaned_col = np.where(mask, col_data, np.nan)\n",
    "        cleaned_data.append(cleaned_col)\n",
    "\n",
    "    # Transpose back to the original shape (n_samples, n_features)\n",
    "    cleaned_data = np.array(cleaned_data).T\n",
    "\n",
    "    # Fill NaN values with the column mean\n",
    "    col_means = np.nanmean(cleaned_data, axis=0)\n",
    "    filled_data = np.where(np.isnan(cleaned_data), col_means, cleaned_data)\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    cov_matrix = np.cov(filled_data, rowvar=False)\n",
    "    return cov_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Data dir and Output dir\n",
    "Data_dir = 'E:/ResidualModel/python/Output_01_fcs_subset_xenith/beads/' #change Data_dir for each dataset\n",
    "#Data_dir = 'E:/ResidualModel/python/Output_01_fcs_subset_xenith/cells/'\n",
    "#Data_dir = 'E:/ResidualModel/python/Output_01_fcs_subset_xenith/external/'\n",
    "#Data_dir = 'E:/ResidualModel/python/Output_01_fcs_subset_Aurora5L/'\n",
    "Output_dir = 'E:/ResidualModel/python/Output_02_calculate_parameters_pinv/Xenith/beads/' #change Output_dir accordingly\n",
    "#Output_dir = 'E:/ResidualModel/python/Output_02_calculate_parameters_pinv/Xenith/cells/'\n",
    "#Output_dir = 'E:/ResidualModel/python/Output_02_calculate_parameters_pinv/Xenith/external/'\n",
    "#Output_dir = 'E:/ResidualModel/python/Output_02_calculate_parameters_pinv/Aurora5L/'\n",
    "Data_list =  os.listdir(Data_dir)\n",
    "insta = \"Xenith\" #Xenith or Aurora5L #change the instumental accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files = [f for f in Data_list if f.endswith('pos.fcs.pkl')]\n",
    "\n",
    "cleaned_names = [f.removesuffix('_pos.fcs.pkl') for f in pos_files]\n",
    "#cleaned_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCC_Cell_PE_KIRDL1\n",
      "Directory 'E:/ResidualModel/extra_17_flow_channel_cor_spectrum/Output/SCC_Cell_PE_KIRDL1' created successfully.\n",
      "residual_mtx\n",
      "(8674, 51)\n",
      "residual_mtx_filtered_high\n",
      "(8587, 51)\n",
      "residual_mtx_filtered_low\n",
      "(8578, 51)\n",
      "residual_mtx_filtered\n",
      "(8454, 51)\n"
     ]
    }
   ],
   "source": [
    "skip = False\n",
    "do_plot = True\n",
    "# do plot will cause much more time. Few hours for the whole dataset.\n",
    "\n",
    "for f in cleaned_names:#cleaned_names[0:31] 62\n",
    "    file_neg = f + \"_neg.fcs.pkl\"\n",
    "    file_pos = f + \"_pos.fcs.pkl\"\n",
    "    #file_sample = f + \"_sample.fcs.pkl\"\n",
    "    file_sample = f + \"_filtered_sample.fcs.pkl\"# for [3, 14, 16, 22, 30]\n",
    "    file_neg_filtered = f + \"_filtered_neg.fcs.pkl\"\n",
    "    #file_neg_filtered = file_neg\n",
    "    print(f)\n",
    "\n",
    "    #get neg signature\n",
    "    path = Data_dir+\"/\"+file_neg\n",
    "    data_neg = pd.read_pickle(path)  \n",
    "\n",
    "    if insta == \"Xenith\":\n",
    "        selected_channels = data_neg.columns[19:70]\n",
    "    elif insta == \"Aurora5L\":\n",
    "        selected_channels = data_neg.columns[list(range(1,17))+list(range(19,35))+list(range(39,71))]\n",
    "\n",
    "    data_neg = data_neg[selected_channels]\n",
    "    data_neg = data_neg.reset_index(drop=True)\n",
    "    neg_sig = data_neg.median()\n",
    "\n",
    "    #get pos signature\n",
    "    path = Data_dir+\"/\"+file_pos\n",
    "    data_pos = pd.read_pickle(path)  \n",
    "\n",
    "    if insta == \"Xenith\":\n",
    "        selected_channels = data_pos.columns[19:70]\n",
    "    elif insta == \"Aurora5L\":\n",
    "        selected_channels = data_pos.columns[list(range(1,17))+list(range(19,35))+list(range(39,71))]\n",
    "\n",
    "    data_pos = data_pos[selected_channels]\n",
    "    data_pos = data_pos.reset_index(drop=True)\n",
    "    pos_sig = data_pos.median()\n",
    "\n",
    "    #get sig\n",
    "    sig = pos_sig - neg_sig\n",
    "    sig = sig / max(sig)\n",
    "    AF_sig = neg_sig / max(neg_sig)\n",
    "\n",
    "    #get data_sample\n",
    "    path = Data_dir+\"/\"+file_sample\n",
    "    data_sample = pd.read_pickle(path)  \n",
    "\n",
    "    if insta == \"Xenith\":\n",
    "        selected_channels = data_sample.columns[19:70]\n",
    "    elif insta == \"Aurora5L\":\n",
    "        selected_channels = data_sample.columns[list(range(1,17))+list(range(19,35))+list(range(39,71))]\n",
    "\n",
    "    data_sample = data_sample[selected_channels]\n",
    "    data_sample = data_sample.reset_index(drop=True)\n",
    "\n",
    "    #get neg filtered signature\n",
    "    path = Data_dir+\"/\"+file_neg_filtered\n",
    "    data_neg_filtered = pd.read_pickle(path)  \n",
    "\n",
    "    if insta == \"Xenith\":\n",
    "        selected_channels = data_neg_filtered.columns[19:70]\n",
    "    elif insta == \"Aurora5L\":\n",
    "        selected_channels = data_neg_filtered.columns[list(range(1,17))+list(range(19,35))+list(range(39,71))]\n",
    "\n",
    "    data_neg_filtered = data_neg_filtered[selected_channels]\n",
    "    data_neg_filtered = data_neg_filtered.reset_index(drop=True)\n",
    "\n",
    "    A_df = pd.concat([neg_sig], axis=1)\n",
    "    A_df.columns = ['AF']\n",
    "    A_pinv = np.linalg.pinv(A_df.values)\n",
    "    unmixed_data = np.dot(A_pinv,np.array(data_neg_filtered).transpose())\n",
    "    explained_data_neg = np.dot(A_df,unmixed_data)\n",
    "    explained_data_neg = np.array(explained_data_neg).transpose()\n",
    "    res_data_neg = data_neg_filtered - explained_data_neg\n",
    "\n",
    "    mean_std_matrix_neg, cov_matrices_neg = para_from_data(res_data_neg)\n",
    "\n",
    "    #fit curve\n",
    "    A_df = pd.concat([sig,neg_sig], axis=1)\n",
    "    A_df.columns = ['target','AF']\n",
    "    A_pinv = np.linalg.pinv(A_df.values)\n",
    "    B_pos = np.dot(A_pinv,np.array(data_pos).transpose())\n",
    "    coef_mtx_pos = np.dot(A_df, B_pos).transpose()\n",
    "    residual_mtx_pos = np.array(data_pos) - coef_mtx_pos\n",
    "\n",
    "    B = np.dot(A_pinv,np.array(data_sample).transpose())\n",
    "    coef_mtx = np.dot(A_df, B).transpose()\n",
    "    residual_mtx = np.array(data_sample) - coef_mtx\n",
    "\n",
    "    A_df = pd.concat([neg_sig], axis=1)\n",
    "    A_df.columns = ['AF']\n",
    "    A_pinv = np.linalg.pinv(A_df.values)\n",
    "    B_neg = np.dot(A_pinv,np.array(data_neg).transpose())\n",
    "    coef_mtx_neg = np.dot(A_df, B_neg).transpose()\n",
    "    residual_mtx_neg = np.array(data_neg) - coef_mtx_neg\n",
    "\n",
    "    tmp_output_dir = Output_dir+\"/\"+f\n",
    "    mkdir_silent(tmp_output_dir)\n",
    "    signature_lineplot(sig,sig_name = f,Output_dir = tmp_output_dir)\n",
    "    signature_lineplot(neg_sig / max(neg_sig),sig_name = f + \"_AF\",Output_dir = tmp_output_dir)\n",
    "\n",
    "    #E1: estimate residual distribution for each channel (positive points (AF + Fluor))\n",
    "    if not skip:\n",
    "        est_pd_pos = plot_pack(file= file_pos,residual_mtx = residual_mtx_pos, data = data_pos, \n",
    "                            selected_channels=selected_channels, Output_dir = tmp_output_dir,\n",
    "                            fit_method = \"norm\",B = np.array(B_pos[0:1,:]))\n",
    "\n",
    "\n",
    "\n",
    "    #E2: estimate residual distribution for each channel (negative points (AF))\n",
    "    if not skip:\n",
    "        est_pd_neg = plot_pack(file= file_neg,residual_mtx = residual_mtx_neg, data = data_neg, \n",
    "                        selected_channels=selected_channels, Output_dir = tmp_output_dir,\n",
    "                        fit_method = \"norm\",B = np.array(B_neg[0:1,:]))#poisson will be more difficult and not practicle, but definitily possible for calculation\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    #E3: estimate residual distribution for each channel (Fluor)\n",
    "    if not skip:\n",
    "        est_pd_fluor = est_pd_pos - est_pd_neg\n",
    "        for i in range(len(selected_channels)):\n",
    "            est_pd_fluor.iloc[1,i] = calculate_sigma_Y(sigma_Z = est_pd_pos.iloc[1,i], sigma_X = est_pd_neg.iloc[1,i])\n",
    "\n",
    "    #E4: estimate residual distribution for each channel from all sample (pos + neg)(positive points (AF + Fluor))\n",
    "        \n",
    "    if f == 'SCC_Cell_SB600_CD244':\n",
    "        mean_std_matrix, cov_matrices, intercepts_B_vs_mean, slopes_B_vs_mean, intercepts_B_vs_std, slopes_B_vs_std, intercepts_B_vs_cov, slopes_B_vs_cov = B_bin_plot_pack(file= file_pos, B= np.array(B[0:1,:]), residual_mtx = residual_mtx, \n",
    "                                                selected_channels=selected_channels, Output_dir= tmp_output_dir, \n",
    "                                                bin_num=10,count_thre = 10,\n",
    "                                                filter_low=0.1,filter_high=95,filter_aroundzero=0.001, do_plot = do_plot)\n",
    "    elif f == 'SCC_Bead_NFR700_CD4':\n",
    "        mean_std_matrix, cov_matrices, intercepts_B_vs_mean, slopes_B_vs_mean, intercepts_B_vs_std, slopes_B_vs_std, intercepts_B_vs_cov, slopes_B_vs_cov = B_bin_plot_pack(file= file_pos, B= np.array(B[0:1,:]), residual_mtx = residual_mtx, \n",
    "                                                selected_channels=selected_channels, Output_dir= tmp_output_dir, \n",
    "                                                bin_num=50,count_thre = 10,\n",
    "                                                filter_low=0.1,filter_high=99,filter_aroundzero=0.01, do_plot = do_plot)\n",
    "    else: \n",
    "        mean_std_matrix, cov_matrices, intercepts_B_vs_mean, slopes_B_vs_mean, intercepts_B_vs_std, slopes_B_vs_std, intercepts_B_vs_cov, slopes_B_vs_cov = B_bin_plot_pack(file= file_pos, B= np.array(B[0:1,:]), residual_mtx = residual_mtx, \n",
    "                                            selected_channels=selected_channels, Output_dir= tmp_output_dir, \n",
    "                                            bin_num=30,count_thre = 10,\n",
    "                                            filter_low=0.1,filter_high=99,filter_aroundzero=0.01, do_plot = do_plot)\n",
    "\n",
    "    #save\n",
    "    if not skip:\n",
    "        est_pd_pos.to_csv(tmp_output_dir + '/est_pd_pos.csv', index=True)\n",
    "        est_pd_neg.to_csv(tmp_output_dir + '/est_pd_neg.csv', index=True)\n",
    "        est_pd_fluor.to_csv(tmp_output_dir + '/est_pd_fluor.csv', index=True)\n",
    "    np.save(tmp_output_dir+\"/mean_std_matrix.npy\", mean_std_matrix)\n",
    "    np.save(tmp_output_dir+\"/cov_matrices.npy\", cov_matrices)\n",
    "    np.save(tmp_output_dir+\"/intercepts_B_vs_mean.npy\", intercepts_B_vs_mean)\n",
    "    np.save(tmp_output_dir+\"/slopes_B_vs_mean.npy\", slopes_B_vs_mean)\n",
    "    np.save(tmp_output_dir+\"/intercepts_B_vs_std.npy\", intercepts_B_vs_std)\n",
    "    np.save(tmp_output_dir+\"/slopes_B_vs_std.npy\", slopes_B_vs_std)\n",
    "    np.save(tmp_output_dir+\"/intercepts_B_vs_cov.npy\", intercepts_B_vs_cov)\n",
    "    np.save(tmp_output_dir+\"/slopes_B_vs_cov.npy\", slopes_B_vs_cov)\n",
    "\n",
    "    sig.to_csv(tmp_output_dir + '/sig.csv', index=True)\n",
    "    neg_sig.to_csv(tmp_output_dir + '/neg_sig.csv', index=True)\n",
    "    pos_sig.to_csv(tmp_output_dir + '/pos_sig.csv', index=True)\n",
    "    np.save(tmp_output_dir+\"/mean_std_matrix_neg.npy\", mean_std_matrix_neg)\n",
    "    np.save(tmp_output_dir+\"/cov_matrices_neg.npy\", cov_matrices_neg)\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will draw some plots to estimate the scc files, which is not necessary for results estimation\n",
    "def old_plot_pack(file,Data_dir,Output_dir, plot_similarity):\n",
    "    file = file\n",
    "    path = Data_dir+\"/\"+file\n",
    "    data = pd.read_pickle(path)  \n",
    "\n",
    "    if insta == \"Xenith\":\n",
    "        selected_channels = data.columns[19:70]\n",
    "    elif insta == \"Aurora5L\":\n",
    "        selected_channels = data.columns[list(range(1,17))+list(range(19,35))+list(range(39,71))]\n",
    "\n",
    "    data = data[selected_channels]\n",
    "    data = data.reset_index(drop=True)\n",
    "    #data[\"Id\"] = range(len(data))\n",
    "    #data = data.loc[0:100,:]\n",
    "    cell_count = len(data)\n",
    "    data = data.transpose()\n",
    "    data.insert(0, \"channel\", pd.Series(selected_channels.tolist(),index=selected_channels))\n",
    "\n",
    "\n",
    "    #plot hist\n",
    "    title = file\n",
    "    data_hist = data.loc[:,1:].transpose()\n",
    "    num_bins = 100\n",
    "    channel = selected_channels[18]\n",
    "\n",
    "    plt.hist(data_hist[channel], num_bins, alpha=1)\n",
    "    plt.title(title+\"\\n\"+channel, loc='left', fontsize=12, fontweight=0)\n",
    "    #plt.xticks(rotation = 90,fontsize = 10)\n",
    "    plt.xlabel(\"Signal Intensity\",fontsize = 10)\n",
    "    plt.ylabel(\"Count\",fontsize = 10)\n",
    "    #plt.show()\n",
    "    plt.savefig(Output_dir+\"/hist_\"+title+\"_\"+channel[0:11]+\".pdf\", format='pdf')  \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    #plot row lineplots\n",
    "    columns = data.columns\n",
    "    title = file\n",
    "    # set figure size\n",
    "    my_dpi=96\n",
    "    plt.figure(figsize=(2500/my_dpi, 1500/my_dpi), dpi=my_dpi)\n",
    "\n",
    "    for column in columns[1:]:\n",
    "        plt.plot(data[column], label = str(column))\n",
    "    plt.title(title, loc='left', fontsize=12, fontweight=0)\n",
    "    plt.xticks(rotation = 90,fontsize = 10)\n",
    "    plt.xlabel(\"Channels\",fontsize = 10)\n",
    "    plt.ylabel(\"Signal\",fontsize = 10)\n",
    "    #plt.show()\n",
    "\n",
    "    plt.savefig(Output_dir+\"/lineplot_\"+title+\".pdf\", format='pdf')  \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    #plot normalized lineplots\n",
    "    data_norm = copy.deepcopy(data)\n",
    "    for column in columns[1:]:\n",
    "        data_norm[column] = data_norm[column]/data_norm[column].max()\n",
    "\n",
    "    columns = data_norm.columns\n",
    "    title = file\n",
    "    # set figure size\n",
    "    my_dpi=96\n",
    "    plt.figure(figsize=(2500/my_dpi, 1500/my_dpi), dpi=my_dpi)\n",
    "\n",
    "    for column in columns[1:]:\n",
    "        plt.plot(data_norm[column], label = str(column))\n",
    "    plt.title(title, loc='left', fontsize=12, fontweight=0)\n",
    "    plt.xticks(rotation = 90,fontsize = 10)\n",
    "    plt.xlabel(\"Channels\",fontsize = 10)\n",
    "    plt.ylabel(\"Normalized Signal\",fontsize = 10)\n",
    "    #plt.show()\n",
    "\n",
    "    plt.savefig(Output_dir+\"/Normalized_lineplot_\"+title+\".pdf\", format='pdf')  \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # prepare for similarity matrix\n",
    "    if plot_similarity:\n",
    "        data_dev = data.loc[:,1:]\n",
    "        for i_row in range(len(data_dev)):\n",
    "            #print(i_row)\n",
    "            data_dev.loc[selected_channels[i_row],:] = data_dev.loc[selected_channels[i_row],:]-np.median(data_dev.loc[selected_channels[i_row],:])\n",
    "\n",
    "        #plot similarity matrix heatmap\n",
    "        cos_sim = cosine_similarity(np.array(data_dev))\n",
    "        df = pd.DataFrame(cos_sim,index=selected_channels,columns=selected_channels)\n",
    "            \n",
    "        #plt.figure(figsize=(fig_weith, fig_height))\n",
    "        sns.set_theme(style=\"ticks\",font_scale=0.5)\n",
    "        p = sns.clustermap(df,cmap=sns.color_palette(\"blend:#ffffff,#365E32\", as_cmap=True), annot=True, fmt=\".2f\",cbar_pos=[0.04, 0.04, 0.02, 0.1],figsize=(16, 16))\n",
    "        p.ax_cbar.set_title('Cosine Similarity')\n",
    "        p.ax_heatmap.set_xlabel('Channels')\n",
    "        p.ax_heatmap.set_ylabel('Channels')\n",
    "        p.ax_heatmap.set_title('Similarity matrix for signal deviation \\n Signal-median(Signal) \\n '+ title, weight=\"bold\", fontsize=10) \n",
    "        p.ax_col_dendrogram.set_visible(False)\n",
    "        plt.savefig(Output_dir+\"/similarity_matrix\"+title+\".pdf\", format='pdf')\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this block will draw some plots to estimate the scc files, which is not necessary for results estimation\n",
    "#plot old_plot_pack\n",
    "for f in cleaned_names[0:62]: #62\n",
    "    file_neg = f + \"_neg.fcs.pkl\"\n",
    "    file_pos = f + \"_pos.fcs.pkl\"\n",
    "\n",
    "    tmp_output_dir = Output_dir+\"/\"+f\n",
    "    mkdir_silent(tmp_output_dir)\n",
    "\n",
    "    old_plot_pack(file=file_pos,Data_dir=Data_dir,Output_dir=tmp_output_dir,plot_similarity = False)\n",
    "    old_plot_pack(file=file_neg,Data_dir=Data_dir,Output_dir=tmp_output_dir,plot_similarity = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "residualModel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
